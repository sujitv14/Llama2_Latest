# -*- coding: utf-8 -*-
"""Llama_Test.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1XZr-mvAYVwrq1eNlw7cJTnUE5KL0mXWJ
"""

pip install ctransformers==0.2.27

pip install langchain==0.2.16

pip install streamlit==1.22.0



from google.colab import drive
drive.mount('/content/drive')

import streamlit as st
from ctransformers import AutoModelForCausalLM
from langchain import PromptTemplate

# Load the model using ctransformers
model_path = '/content/drive/MyDrive/llama-2-7b-chat.ggmlv3.q8_0.bin'
llm = AutoModelForCausalLM.from_pretrained(model_path, model_type="llama")

# Define the prompt template
prompt = PromptTemplate(template="Question: {question}\nAnswer:", input_variables=["question"])

# Use the new RunnableSequence syntax
chain = prompt | llm

# Streamlit UI for input
st.title("LLaMA-2 Chatbot")
question = st.text_input("Enter your question:")

if st.button("Generate Answer"):
    # Run inference
    response = chain.invoke({"question": question})
    st.write(f"Answer: {response}")